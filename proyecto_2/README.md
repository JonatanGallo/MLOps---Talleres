# ğŸ§ MLOps - Talleres Clasificador de PingÃ¼inos

Este repositorio contiene el cÃ³digo usado para entrenar y desplegar un modelo de predicciÃ³n de clases de pingÃ¼inos basado en el dataset disponible en: [palmerpenguins](https://pypi.org/project/palmerpenguins/). El proyecto abarca desde la preparaciÃ³n de datos y el entrenamiento de modelos hasta el desplieque de una API REST para realizar predicciones.

---

## Arquitectura de Servicios

- **Servicio de PredicciÃ³n**: API REST con FastAPI para inferencia de modelos
- **OrquestaciÃ³n de Workflows**: Apache Airflow para automatizaciÃ³n y programaciÃ³n de tareas de ML
- **Servicio de Entrenamiento**: AplicaciÃ³n Python para entrenamiento automatizado de modelos
- **Base de Datos**: MySQL para almacenamiento de datos de entrenamiento
- **VolÃºmenes Compartidos**: Sistema de archivos compartido para modelos entrenados y logs

### Componentes de Airflow

- **Airflow Webserver**: Interfaz web para monitoreo y gestiÃ³n de DAGs (puerto 8080)
- **Airflow Scheduler**: Planificador de tareas que ejecuta los DAGs segÃºn su programaciÃ³n
- **Airflow Worker**: Ejecutor de tareas usando Celery
- **Airflow Triggerer**: Manejo de sensores y triggers asÃ­ncronos
- **PostgreSQL**: Base de datos de metadatos de Airflow
- **Redis**: Broker de mensajes para Celery

## Entrenamiento de Modelos

### Workflow Automatizado con Airflow

El entrenamiento de modelos se ejecuta mediante un DAG (Directed Acyclic Graph) de Airflow que automatiza todo el proceso:

1. **clear_raw_data**: Limpia datos anteriores de la base de datos
2. **store_raw_data**: Descarga y almacena datos frescos del dataset de pingÃ¼inos
3. **get_raw_data**: Extrae y procesa los datos para entrenamiento
4. **save_all_models**: Entrena y guarda todos los modelos (Random Forest, SVM, Neural Network)

### CaracterÃ­sticas del Entrenamiento

- **AutomatizaciÃ³n completa**: El DAG se ejecuta segÃºn la programaciÃ³n definida
- **Soporte para mÃºltiples tipos de modelos** con selecciÃ³n dinÃ¡mica
- **Persistencia automÃ¡tica** de modelos entrenados en el volumen compartido
- **Monitoreo en tiempo real** a travÃ©s de la interfaz web de Airflow
- **Notebook interactivo** (TrainModels.ipynb) disponible para experimentaciÃ³n manual

## Mejoras en la API

- Endpoint para listado de modelos disponibles

- Sistema de normalizaciÃ³n de datos de entrada

- Manejo de errores mejorado

- DocumentaciÃ³n interactiva automÃ¡tica

## CreaciÃ³n del modelo

- El dataset se carga usando el mÃ©todo `load_penguins` expuesto en la librerÃ­a del proyecto `palmerpenguins`.  
- Se convierten columnas categÃ³ricas a numÃ©ricas usando One-shot Encoding.
- Se hace una escala para mantener la desviaciÃ³n estandar por debajo de 1.
- Se eliminan caracterÃ­sticas no representativas para los modelos(year).
---

## CaracterÃ­sticas principales

- ğŸ§ ETL completo para preparaciÃ³n de datos  
- ğŸ¤– Entrenamiento de 3 modelos de ML diferentes  
- ğŸš€ API REST con FastAPI para predicciones  
- ğŸ“¦ DockerizaciÃ³n con compose para despliegue multi-servicio
- ğŸ”„ Sistema dinÃ¡mico de selecciÃ³n de modelos  
- ğŸ“Š JupyterLab integrado para experimentaciÃ³n
- ğŸ” Interfaz de documentaciÃ³n automÃ¡tica
- âš¡ **OrquestaciÃ³n con Apache Airflow** para automatizaciÃ³n de workflows
- ğŸ—„ï¸ **Base de datos MySQL** para persistencia de datos de entrenamiento
- ğŸ“ˆ **Monitoreo en tiempo real** de tareas de ML

---

## InstalaciÃ³n y configuraciÃ³n

Clonar el repositorio:

```bash
git clone https://github.com/JonatanGallo/MLOps---Talleres.git
cd penguins-taller-3-airflow
```

## EjecuciÃ³n con Docker Compose

ConstrucciÃ³n y ejecuciÃ³n de todos los servicios:

```bash
docker-compose up
```

## Servicios desplegados: 

- **API de PredicciÃ³n**: http://localhost:8012
- **Airflow Webserver**: http://localhost:8080 (usuario: airflow, contraseÃ±a: airflow)
- **MySQL Database**: localhost:3306 (usuario: user, contraseÃ±a: password, base de datos: training)

## Entrenamiento de modelos

### Entrenamiento Automatizado con Airflow

1. **Acceder a Airflow**: Abrir http://localhost:8080 en el navegador
2. **Credenciales**: Usuario: `airflow`, ContraseÃ±a: `airflow`
3. **Ejecutar DAG**: Buscar el DAG `training_dag` y activarlo
4. **Monitorear**: Ver el progreso de las tareas en tiempo real

### Entrenamiento Manual (Opcional)

Para experimentaciÃ³n manual, se puede usar el notebook disponible:

```python
# Ejemplo de entrenamiento desde el notebook
from etl import get_data
from model import Model
from models import ModelType

X, y = get_data()
model = Model(ModelType.RANDOM_FOREST)
model.train(X, y)
model.save('/models/model_random_forest.pkl')
```
---

## Uso de la API


### 1. Acceder a la interfaz de documentaciÃ³n

Abrir en el navegador:  
[http://localhost:8012/docs](http://localhost:8012/docs)

---

## Uso de la API

### Listar modelos disponibles

```bash
curl http://localhost:8012/models
```

Respuesta:

```json
{
  "available_models": [
    "random_forest",
    "svm",
    "neural_network"
  ]
}
```
### Cargar un modelo especÃ­fico

```bash
curl http://localhost:8012/load_model/random_forest
```

### Predecir con selecciÃ³n dinÃ¡mica de modelo (POST)

```bash
curl -X POST "http://localhost:8012/predict/random_forest" \
-H "Content-Type: application/json" \
-d '{
  "island": "Biscoe",
  "bill_length_mm": 50.0,
  "bill_depth_mm": 16.3,
  "flipper_length_mm": 230,
  "body_mass_g": 5700,
  "sex": "male",
  "year": 2007
}'
```

Respuesta:

```json
{
  "model_used": "random_forest",
  "prediction": "Gentoo"
}
```

---

## Estructura del proyecto

```
penguins-taller-3-airflow/
â”œâ”€â”€ api/                         # Servicio de API de predicciÃ³n
â”‚   â”œâ”€â”€ main.py                  # AplicaciÃ³n FastAPI principal
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â”œâ”€â”€ penguins.py             # DefiniciÃ³n de especies de pingÃ¼inos
â”‚   â”œâ”€â”€ uv.lock
â”‚   â”œâ”€â”€ ModelService.py          # Servicio para manejar modelos
â”‚   â”œâ”€â”€ Dockerfile              # Dockerfile para el servicio de API
â”‚   â””â”€â”€ dto/                    # Objetos de transferencia de datos
â”‚       â”œâ”€â”€ model_prediction_request.py
â”‚       â””â”€â”€ normalized_request.py
â”œâ”€â”€ training-app/               # Servicio de entrenamiento
â”‚   â”œâ”€â”€ TrainModels.ipynb       # Notebook de entrenamiento
â”‚   â”œâ”€â”€ etl.py                  # ExtracciÃ³n, transformaciÃ³n y carga
â”‚   â”œâ”€â”€ model.py                # DefiniciÃ³n de modelos de ML
â”‚   â”œâ”€â”€ models.py               # Tipos de modelos disponibles
â”‚   â”œâ”€â”€ train.py                # Script de entrenamiento automatizado
â”‚   â”œâ”€â”€ db.py                   # ConexiÃ³n a base de datos
â”‚   â”œâ”€â”€ pyproject.toml          # ConfiguraciÃ³n de dependencias
â”‚   â”œâ”€â”€ uv.lock
â”‚   â”œâ”€â”€ Dockerfile              # Dockerfile para el servicio de entrenamiento
â”‚   â””â”€â”€ raw_data.csv            # Datos de entrenamiento
â”œâ”€â”€ dags/                       # DAGs de Airflow
â”‚   â”œâ”€â”€ training.py             # DAG principal de entrenamiento
â”‚   â””â”€â”€ training_app/           # MÃ³dulos compartidos con el DAG
â”œâ”€â”€ airflow/                    # ConfiguraciÃ³n de Airflow
â”‚   â”œâ”€â”€ Dockerfile              # Dockerfile personalizado de Airflow
â”‚   â””â”€â”€ requirements.txt        # Dependencias adicionales
â”œâ”€â”€ models/                     # Modelos entrenados (volumen compartido)
â”‚   â”œâ”€â”€ model_neural_network.pkl
â”‚   â”œâ”€â”€ model_random_forest.pkl
â”‚   â””â”€â”€ model_svm.pkl
â”œâ”€â”€ logs/                       # Logs de Airflow
â”œâ”€â”€ plugins/                    # Plugins personalizados de Airflow
â”œâ”€â”€ docker-compose.yml          # OrquestaciÃ³n de servicios
â””â”€â”€ README.md                   
```

---

## Workflow del DAG de Entrenamiento

El DAG `training_dag` automatiza el proceso completo de entrenamiento de modelos:

### Tareas del DAG

1. **clear_raw_data** ğŸ—‘ï¸
   - Limpia datos anteriores de la base de datos MySQL
   - Prepara el entorno para nuevos datos

2. **store_raw_data** ğŸ“¥
   - Descarga el dataset de pingÃ¼inos desde la fuente
   - Almacena los datos en la base de datos MySQL

3. **get_raw_data** ğŸ”„
   - Extrae datos de la base de datos
   - Aplica transformaciones ETL (One-hot encoding, escalado)
   - Prepara los datos para entrenamiento

4. **save_all_models** ğŸ¤–
   - Entrena los 3 modelos: Random Forest, SVM, Neural Network
   - Guarda los modelos entrenados en el volumen compartido
   - Persiste el scaler para normalizaciÃ³n

### Flujo de EjecuciÃ³n

```
clear_raw_data â†’ store_raw_data â†’ get_raw_data â†’ save_all_models
```

### ProgramaciÃ³n

- **Schedule**: `@once` (ejecuciÃ³n manual)
- **Start Date**: 2023-05-01
- **Dependencias**: Cada tarea depende de la anterior

---

## Modelos implementados

- Random Forest - Clasificador de bosques aleatorios  
- SVM - MÃ¡quinas de Soporte Vectorial  
- Neural Network - Red neuronal multicapa  

---


## Especies de pingÃ¼inos soportadas

| Especie    | Valor numÃ©rico | DescripciÃ³n             |
|------------|----------------|-------------------------|
| Adelie     | 0              | PingÃ¼inos Adelia        |
| Chinstrap  | 1              | PingÃ¼inos de barbijo    |
| Gentoo     | 2              | PingÃ¼inos papÃºa         |

---
## Desarrollo

Para desarrollo y debugging:

- **Ejecutar servicios**: `docker-compose up` para iniciar todos los servicios
- **Monitorear Airflow**: Acceder a http://localhost:8080 (usuario: airflow, contraseÃ±a: airflow)
- **Ejecutar DAGs**: Activar y ejecutar el DAG `training_dag` desde la interfaz web
- **Ver logs**: Monitorear el progreso de las tareas en tiempo real
- **Modificar cÃ³digo**: Los cambios en `training-app/` se reflejan automÃ¡ticamente
- **Modelos**: Los modelos entrenados se guardan en el directorio `models/` compartido
- **Base de datos**: Conectar a MySQL en localhost:3306 para inspeccionar datos

---
## Notas de la versiÃ³n

- **Modelos**: Se persisten en el volumen compartido `models/`
- **Airflow**: Incluye interfaz web completa para monitoreo y gestiÃ³n de DAGs
- **Base de datos**: MySQL integrado para persistencia de datos de entrenamiento
- **API**: Se recarga automÃ¡ticamente durante el desarrollo
- **AutomatizaciÃ³n**: Los modelos se entrenan automÃ¡ticamente mediante el DAG de Airflow
- **Monitoreo**: Logs detallados disponibles en la interfaz de Airflow

---

## Diagrama de Arquitectura
![Alt text](./arquitecturaAirflow.drawio.svg)
