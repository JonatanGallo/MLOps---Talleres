# Taller 5: Pruebas de Carga y OptimizaciÃ³n de API con Locust

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto tiene como objetivo realizar pruebas de carga a una API de inferencia de Machine Learning utilizando **Locust**, una herramienta de cÃ³digo abierto para pruebas de carga distribuidas. La API estÃ¡ construida con **FastAPI** y utiliza **MLflow** para servir modelos de predicciÃ³n.

El proyecto se compone de dos componentes principales:

1. **API de Inferencia**: Servicio FastAPI que expone un endpoint `/predict` para realizar predicciones de cobertura forestal utilizando un modelo entrenado y almacenado en MLflow.

2. **Locust**: Herramienta de pruebas de carga configurada en modo maestro-trabajador (master-worker) para simular mÃºltiples usuarios concurrentes realizando solicitudes a la API.

### Estructura del Proyecto

```
taller-5-locust/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ docker-compose.yml          # ConfiguraciÃ³n de la API de inferencia
â”œâ”€â”€ locust/
â”‚   â”œâ”€â”€ docker-compose.yml          # ConfiguraciÃ³n de Locust (master y workers)
â”‚   â”œâ”€â”€ Dockerfile                  # Imagen personalizada de Locust
â”‚   â”œâ”€â”€ locustfile.py              # Script de pruebas de carga
â”‚   â””â”€â”€ requirements.txt           # Dependencias de Locust
â”œâ”€â”€ images/                         # Capturas de pantalla de los resultados
â””â”€â”€ README.md
```

## ğŸ¯ Objetivo

El objetivo principal de este taller es identificar cuellos de botella en la API de inferencia y aplicar diferentes estrategias de optimizaciÃ³n para mejorar el rendimiento, medido en tÃ©rminos de:

- **RPS (Requests Per Second)**: NÃºmero de solicitudes que la API puede manejar por segundo
- **Latencia**: Tiempo de respuesta promedio
- **Tasa de fallos**: Porcentaje de solicitudes que fallan

---

## ğŸš€ Proceso de OptimizaciÃ³n

A continuaciÃ³n se detalla el proceso completo de optimizaciÃ³n seguido en este proyecto, con anÃ¡lisis y conclusiones en cada etapa.

### 1. EliminaciÃ³n de Prints en la API

**Problema inicial**: La API contenÃ­a mÃºltiples declaraciones `print()` para debugging que impactaban negativamente el rendimiento de la aplicaciÃ³n.

**SoluciÃ³n**: Se eliminaron todos los `print()` innecesarios del cÃ³digo de la API.

**Resultado**: 
- **Antes**: La API solo alcanzaba **~4 RPS** (Requests Per Second)
- **DespuÃ©s**: Se logrÃ³ una mejora de hasta **~120 RPS** significativa en el rendimiento inicial

**LecciÃ³n aprendida**: Los `print()` son operaciones de I/O sÃ­ncronas que bloquean la ejecuciÃ³n y pueden degradar significativamente el rendimiento en aplicaciones de producciÃ³n, especialmente bajo alta carga. Es fundamental utilizar sistemas de logging apropiados y con niveles adecuados en producciÃ³n.

---

### 1.5. EliminaciÃ³n de Llamadas Innecesarias a MLflow

**Problema identificado**: Se detectaron llamadas redundantes a MLflow que no eran estrictamente necesarias para el proceso de inferencia.

**SoluciÃ³n**: Se optimizÃ³ el cÃ³digo para eliminar las llamadas duplicadas o innecesarias a MLflow, manteniendo solo las operaciones esenciales para la predicciÃ³n.

**Resultado**: ReducciÃ³n adicional en la latencia de las peticiones al minimizar la comunicaciÃ³n con servicios externos.

**LecciÃ³n aprendida**: Cada llamada a servicios externos (MLflow, bases de datos, APIs) aÃ±ade latencia. Es crucial revisar el cÃ³digo para identificar y eliminar operaciones redundantes o que puedan ser cacheadas.

---

### 2. AnÃ¡lisis Comparativo de Endpoints

Para identificar especÃ­ficamente dÃ³nde se encontraban los cuellos de botella, se realizÃ³ una comparaciÃ³n entre dos endpoints:

- **Endpoint vacÃ­o**: Un endpoint simple que solo devuelve un string sin procesamiento
- **Endpoint de predicciÃ³n**: El endpoint `/predict` que realiza todo el proceso de inferencia

#### 2.1 Resultados y Conclusiones

![ComparaciÃ³n entre endpoint vacÃ­o y endpoint de predicciÃ³n](./images/endpointVacio.png)
*Figura 1: Rendimiento del endpoint vacÃ­o*

![Rendimiento del endpoint de predicciÃ³n](./images/prediccionSinConBD.png)
*Figura 2: Rendimiento del endpoint de predicciÃ³n con llamadas a BD y MLflow*

**Conclusiones del anÃ¡lisis**:

Se identificaron **dos cuellos de botella principales** en el proceso de predicciÃ³n:

1. **Llamada a la base de datos**: La API realizaba una consulta a la base de datos en cada peticiÃ³n para obtener los nombres de las columnas necesarias para el modelo. Esta operaciÃ³n aÃ±adÃ­a latencia significativa.

2. **Llamada a MLflow**: El proceso de inferencia requerÃ­a comunicarse con MLflow para cargar el modelo y realizar la predicciÃ³n. Esta operaciÃ³n remota era el cuello de botella mÃ¡s grande, ya que:
   - Implica comunicaciÃ³n de red con el servidor de MLflow
   - Requiere cargar el modelo en cada inferencia
   - El proceso de inferencia en sÃ­ mismo toma tiempo

**Prueba sin conexiones externas**:

Para validar el impacto real de las llamadas a la base de datos y MLflow, se realizÃ³ una prueba eliminando completamente estas dependencias:

- **ConfiguraciÃ³n**: Endpoint de prueba que solo normaliza datos sin consultar BD ni MLflow
- **Resultado**: Se alcanzaron aproximadamente **~5000 RPS**
- **ConclusiÃ³n**: Las llamadas a servicios externos (BD y MLflow) son responsables de mÃ¡s del **95% de la latencia** en el proceso de inferencia

Esta prueba confirmÃ³ que el mayor cuello de botella estÃ¡ en las dependencias externas, no en el procesamiento interno de la API.

![AnÃ¡lisis de tiempos](./images/soloNormalizarDatos.png)
*Figura 3: AnÃ¡lisis detallado de los tiempos de procesamiento*

---

### 3. ImplementaciÃ³n de RÃ©plicas y Workers con Docker Swarm

**Estrategia**: Para aumentar la capacidad de procesamiento de la API, se implementÃ³ una arquitectura distribuida utilizando:

- **RÃ©plicas de la API**: MÃºltiples instancias del contenedor de la API ejecutÃ¡ndose en paralelo
- **Workers de Uvicorn**: MÃºltiples procesos workers dentro de cada contenedor
- **Docker Swarm**: OrquestaciÃ³n de contenedores para distribuir la carga

#### ConfiguraciÃ³n implementada

```yaml
# API con mÃºltiples workers
command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

```yaml
# Locust en modo distribuido
locust-master:
  command: -f /mnt/locustfile.py --master --expect-workers 8

locust-worker:
  command: -f /mnt/locustfile.py --worker --master-host locust-master
  deploy: {}  # Escalable con: docker compose up --scale locust-worker=8
```

#### Arquitectura Master-Worker de Locust

Una de las optimizaciones clave fue implementar una arquitectura distribuida de Locust usando el patrÃ³n **Master-Worker**:

**Componentes**:

1. **Locust Master**: 
   - Coordina la ejecuciÃ³n de las pruebas
   - Gestiona la interfaz web (puerto 8089)
   - Distribuye la carga entre los workers
   - Agrega las estadÃ­sticas de todos los workers

2. **Locust Workers**:
   - Ejecutan las pruebas de carga reales
   - Se conectan al master
   - Cada worker simula usuarios independientemente
   - Escalables horizontalmente con `--scale`

**ConfiguraciÃ³n del docker-compose de Locust**:

```yaml
locust-master:
  image: locustio/locust
  command: -f /mnt/locustfile.py --master --expect-workers 8
  ports:
    - "8010:8089"
  environment:
    - LOCUST_HOST=http://10.43.100.86:8006
  volumes:
    - ./locustfile.py:/mnt/locustfile.py
  ulimits:
    nofile:
      soft: 1048576
      hard: 1048576

locust-worker:
  image: locustio/locust
  command: -f /mnt/locustfile.py --worker --master-host locust-master
  environment:
    - LOCUST_HOST=http://10.43.100.86:8006
  depends_on: [locust-master]
  volumes:
    - ./locustfile.py:/mnt/locustfile.py
  ulimits:
    nofile:
      soft: 1048576
      hard: 1048576
```

**Escalado de workers**:
```bash
# Levantar Locust con 8 workers
docker compose up --scale locust-worker=8
```

**Ventajas de esta arquitectura**:
- âœ… **Escalabilidad**: Permite simular miles de usuarios concurrentes
- âœ… **DistribuciÃ³n de carga**: Los workers distribuyen la carga de forma equilibrada
- âœ… **Flexibilidad**: Se pueden agregar/quitar workers dinÃ¡micamente
- âœ… **Mejor utilizaciÃ³n de recursos**: Aprovecha mÃºltiples cores del CPU

#### Optimizaciones en Locust

Para maximizar el rendimiento de las pruebas de carga, se aplicaron las siguientes configuraciones en el archivo `locustfile.py`:

```python
class UsuarioDeCarga(FastHttpUser):
    wait_time = between(4, 9)
    connections = 800         # Pool grande de conexiones por worker
    max_reqs_per_conn = 0    # Sin lÃ­mite de requests por conexiÃ³n
```

**Mejoras implementadas**:
- **FastHttpUser**: Uso de cliente HTTP optimizado basado en `geventhttpclient`, mÃ¡s rÃ¡pido que el cliente estÃ¡ndar
- **Pool de conexiones grande (800)**: Permite reutilizar conexiones TCP, reduciendo el overhead de crear nuevas conexiones
- **Sin lÃ­mite de requests**: Maximiza la reutilizaciÃ³n de conexiones al no cerrarlas despuÃ©s de un nÃºmero fijo de peticiones
- **ulimits aumentados**: Se incrementÃ³ el lÃ­mite de archivos abiertos a 1048576 para soportar miles de conexiones simultÃ¡neas

![Rendimiento con 4 rÃ©plicas sin workers adicionales](./images/4replicasSinWorkers.png)
*Figura 4: Rendimiento con 4 rÃ©plicas de la API*

![Rendimiento con 8 workers de Locust](./images/8workers.png)
*Figura 5: ConfiguraciÃ³n con 8 workers de Locust*

**Resultados**:

- âœ… **Mejora significativa**: El uso de rÃ©plicas y workers incrementÃ³ considerablemente el RPS y la capacidad de manejar usuarios concurrentes
- âš ï¸ **LimitaciÃ³n persistente**: A pesar de la mejora, el cuello de botella de las llamadas a MLflow sigue siendo un factor limitante importante
- ğŸ“ˆ **Escalabilidad horizontal**: Se demostrÃ³ que la API puede escalar horizontalmente, pero con rendimientos decrecientes debido a las dependencias externas

**DesafÃ­os encontrados**:

- ConfiguraciÃ³n de Docker Swarm requiere networking apropiado entre nodos
- Balance de carga entre rÃ©plicas debe considerar el estado de conexiones de MLflow
- Los recursos del sistema host se convierten en el nuevo lÃ­mite

#### SeparaciÃ³n de Infraestructura: API y Locust en MÃ¡quinas Independientes

**DecisiÃ³n arquitectÃ³nica crÃ­tica**: Para obtener mÃ©tricas precisas y evitar la competencia por recursos del sistema, se desplegaron la API y Locust en **mÃ¡quinas separadas**.

**ConfiguraciÃ³n del despliegue**:

```
MÃ¡quina 1 (Servidor API):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IP: 10.43.100.86:8006         â”‚
â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   API Container (Swarm)  â”‚  â”‚
â”‚  â”‚   - 4 rÃ©plicas           â”‚  â”‚
â”‚  â”‚   - 4 workers c/u        â”‚  â”‚
â”‚  â”‚   - 600MB-1GB RAM c/u    â”‚  â”‚
â”‚  â”‚   - 1-2 CPU c/u          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MÃ¡quina 2 (Servidor Locust):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IP: localhost:8010             â”‚
â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Locust Master          â”‚  â”‚
â”‚  â”‚   - Puerto 8089 (web)    â”‚  â”‚
â”‚  â”‚   - Coordina workers     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Locust Workers (x8)    â”‚  â”‚
â”‚  â”‚   - Generan carga        â”‚  â”‚
â”‚  â”‚   - 800 conexiones c/u   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Beneficios de la separaciÃ³n**:

1. **Aislamiento de recursos**:
   - La API usa 100% de los recursos de su mÃ¡quina sin competencia
   - Locust no afecta el rendimiento medido de la API
   - MÃ©tricas mÃ¡s precisas y confiables

2. **Escalabilidad independiente**:
   - Se pueden agregar mÃ¡s recursos a la API sin afectar Locust
   - Se pueden escalar los workers de Locust sin limitar la API

3. **SimulaciÃ³n realista**:
   - Refleja un escenario de producciÃ³n donde los clientes estÃ¡n en mÃ¡quinas distintas
   - Incluye latencia de red real entre cliente y servidor

4. **Debugging facilitado**:
   - MÃ¡s fÃ¡cil identificar si los problemas son de la API o del generador de carga
   - Monitoreo independiente de recursos (CPU, RAM, red) en cada mÃ¡quina

**Resultados de la separaciÃ³n**:
- âœ… MÃ©tricas mÃ¡s precisas y reproducibles
- âœ… Capacidad de generar mayor carga sin saturar la mÃ¡quina de la API
- âœ… Mejor identificaciÃ³n de los verdaderos cuellos de botella
- âœ… Posibilidad de monitorear recursos de forma independiente

---

### 4. AnÃ¡lisis de AsignaciÃ³n de Recursos

Se realizaron mÃºltiples pruebas variando los recursos asignados a cada rÃ©plica de la API para encontrar la configuraciÃ³n Ã³ptima.

#### Configuraciones evaluadas

| RÃ©plicas | Memoria | CPU | Workers Uvicorn | RPS | Observaciones |
|----------|---------|-----|-----------------|-----|---------------|
| 1 | 2GB | 1 | 2 | ~X | Baseline |
| 2 | 1GB | 1 | 2 | ~X | Bueno |
| 4 | 600MB | 1 | 4 | ~X | Ã“ptimo |
| 4 | 1GB | 2 | 4 | ~X | Mejor rendimiento |

![AnÃ¡lisis de recursos 1](./images/recursos-config-1.png)
*Figura 6: Prueba de asignaciÃ³n de recursos - ConfiguraciÃ³n 1*

![AnÃ¡lisis de recursos 2](./images/recursos-config-2.png)
*Figura 7: Prueba de asignaciÃ³n de recursos - ConfiguraciÃ³n 2*

![AnÃ¡lisis de recursos 3](./images/recursos-config-3.png)
*Figura 8: Prueba de asignaciÃ³n de recursos - ConfiguraciÃ³n 3*

![AnÃ¡lisis de recursos 4](./images/recursos-config-4.png)
*Figura 9: Prueba de asignaciÃ³n de recursos - ConfiguraciÃ³n 4*

![AnÃ¡lisis de recursos 5](./images/recursos-config-5.png)
*Figura 10: Prueba de asignaciÃ³n de recursos - ConfiguraciÃ³n 5*

#### Conclusiones de la asignaciÃ³n de recursos

**ConfiguraciÃ³n Ã³ptima identificada**:

- **Memoria**: Entre **600MB - 1GB** por rÃ©plica
- **CPU**: Entre **1-2 CPUs** por rÃ©plica
- **Workers**: La cantidad Ã³ptima de workers depende de:
  - NÃºmero de CPUs asignados
  - Memoria disponible
  - Tipo de carga de trabajo (IO-bound vs CPU-bound)

**Recomendaciones**:

1. **Para cargas IO-bound** (como esta API con llamadas a MLflow):
   - Usar mÃ¡s workers (2-4 por CPU)
   - La memoria es mÃ¡s importante que CPUs adicionales

2. **Balanceo workers vs rÃ©plicas**:
   - MÃ¡s rÃ©plicas con pocos workers: Mejor aislamiento, mayor overhead
   - Pocas rÃ©plicas con muchos workers: Mejor uso de recursos, menor overhead

3. **Monitoreo continuo**:
   - Observar uso de CPU y memoria en tiempo real
   - Ajustar segÃºn patrones de trÃ¡fico reales
   - Considerar auto-scaling basado en mÃ©tricas

---

## ğŸ³ PublicaciÃ³n en Docker Hub

Como parte del proyecto, se publicÃ³ una imagen Docker de la API optimizada en Docker Hub para facilitar su despliegue:

**Imagen publicada**: `jdromero9402/mlops_talleres:inference`

Esta imagen contiene:
- API FastAPI optimizada con todas las mejoras de rendimiento
- Dependencias necesarias para conectar con MLflow
- ConfiguraciÃ³n lista para ejecutarse en producciÃ³n

Para usar la imagen:
```bash
docker pull jdromero9402/mlops_talleres:inference
docker run -p 8000:8000 jdromero9402/mlops_talleres:inference
```

---

## ğŸ”§ ConfiguraciÃ³n y EjecuciÃ³n

### Prerrequisitos

- Docker y Docker Compose instalados en ambas mÃ¡quinas
- Imagen de la API: `jdromero9402/mlops_talleres:inference` (publicada en Docker Hub)
- ConexiÃ³n a MLflow configurada
- Conectividad de red entre ambas mÃ¡quinas

### Arquitectura de despliegue

**MÃ¡quina 1 - Servidor de API:**
```bash
# En la mÃ¡quina del servidor (ej: 10.43.100.86)
cd api
docker-compose up -d

# O con Docker Swarm para mÃºltiples rÃ©plicas
docker swarm init
docker stack deploy -c docker-compose.yml api-stack
```

**MÃ¡quina 2 - Servidor de Locust:**
```bash
# En la mÃ¡quina de pruebas de carga (diferente a la API)
cd locust

# Modo distribuido (con 8 workers recomendado)
docker-compose up --scale locust-worker=8
```

### Acceder a la interfaz de Locust

Desde cualquier navegador:
- URL: `http://<IP-MAQUINA-LOCUST>:8010`
- Ejemplo: `http://localhost:8010` (si estÃ¡s en la mÃ¡quina de Locust)

### Configurar prueba de carga

En la interfaz web de Locust:
1. **Number of users**: Cantidad de usuarios concurrentes a simular (ej: 1000)
2. **Spawn rate**: Usuarios nuevos por segundo (ej: 50)
3. **Host**: URL de la API en la otra mÃ¡quina: `http://10.43.100.86:8006`

### Comandos Ãºtiles

```bash
# Ver logs del master de Locust
docker-compose logs -f locust-master

# Ver estadÃ­sticas de workers
docker-compose ps

# Escalar workers dinÃ¡micamente
docker-compose up --scale locust-worker=16 -d

# Detener las pruebas
docker-compose down
```

---

## ğŸ“Š MÃ©tricas y Resultados

### MÃ©tricas clave monitoreadas

- **RPS (Requests Per Second)**: Throughput de la API
- **Response Time (ms)**: Latencia promedio, percentil 95, percentil 99
- **Failure Rate (%)**: Porcentaje de peticiones fallidas
- **Number of Users**: Usuarios concurrentes soportados
- **CPU/Memory Usage**: Uso de recursos del sistema

### EvoluciÃ³n del rendimiento

| Etapa | OptimizaciÃ³n aplicada | RPS aproximado | Mejora |
|-------|----------------------|----------------|--------|
| Inicial | Ninguna (con prints) | ~4 | Baseline |
| 1 | Sin prints | ~15-20 | +275-400% |
| 1.5 | Sin llamadas innecesarias a MLflow | ~25-30 | +525-650% |
| 2 | AnÃ¡lisis de bottlenecks | - | IdentificaciÃ³n |
| 2.1 | **Prueba sin BD/MLflow** | **~5000** | **+124900%** |
| 3 | RÃ©plicas + Workers | ~50-80 | +1150-1900% |
| 4 | Recursos optimizados + Locust optimizado | ~80-120 | +1900-2900% |

*Nota: Los valores son aproximados y dependen de la configuraciÃ³n especÃ­fica y recursos del sistema.*

**Dato importante**: La prueba sin conexiones a base de datos ni MLflow demostrÃ³ que la API puede manejar hasta **~5000 RPS**, lo que indica que el procesamiento interno no es el cuello de botella, sino las llamadas a servicios externos.

---

## ğŸ“ Lecciones Aprendidas

### 1. OptimizaciÃ³n progresiva
- Comenzar con mejoras simples (eliminar prints) que dan resultados inmediatos
- Medir despuÃ©s de cada cambio para validar el impacto
- Las optimizaciones mÃ¡s complejas no siempre dan los mejores resultados

### 2. IdentificaciÃ³n de cuellos de botella
- Es fundamental medir y comparar para identificar dÃ³nde estÃ¡n realmente los problemas
- Los endpoints de prueba simples ayudan a aislar componentes problemÃ¡ticos
- Las dependencias externas (BD, MLflow) suelen ser los mayores cuellos de botella

### 3. Escalabilidad horizontal
- Docker Swarm y rÃ©plicas permiten escalar efectivamente
- Sin embargo, si hay cuellos de botella compartidos (MLflow), la escalabilidad es limitada
- El balance entre rÃ©plicas y workers debe ajustarse al tipo de carga
- La arquitectura Master-Worker de Locust es fundamental para generar carga realista a gran escala

### 4. AsignaciÃ³n de recursos
- MÃ¡s recursos no siempre significa mejor rendimiento
- Existe un punto Ã³ptimo de recursos que maximiza el costo-beneficio
- El tipo de carga (IO-bound vs CPU-bound) determina quÃ© recursos son mÃ¡s crÃ­ticos

### 5. Monitoreo y testing
- Locust es una herramienta poderosa para simular carga realista
- El modo distribuido master-worker permite simular miles de usuarios
- Es importante realizar pruebas con diferentes patrones de carga

### 6. SeparaciÃ³n de infraestructura
- Desplegar servicios de prueba (Locust) y servicios bajo prueba (API) en mÃ¡quinas separadas es crucial
- Evita la competencia por recursos que puede distorsionar las mÃ©tricas
- Permite mediciones mÃ¡s precisas y representativas del rendimiento real
- Facilita el debugging al aislar problemas de cada componente

---

## ğŸ”® Mejoras Futuras

### Optimizaciones de arquitectura

1. **Implementar cachÃ© de modelos**
   - Cargar el modelo de MLflow una sola vez al iniciar la aplicaciÃ³n
   - Mantener el modelo en memoria para todas las inferencias
   - Implementar cache warming al desplegar nuevas rÃ©plicas
   - **Impacto esperado**: Acercarse a los ~5000 RPS demostrados en las pruebas sin MLflow

2. **CachÃ© de metadata**
   - Almacenar nombres de columnas en Redis o en memoria
   - Eliminar completamente las consultas a BD en el hot path
   - Actualizar cachÃ© solo cuando el schema cambie

3. **OptimizaciÃ³n de inferencia**
   - Batch predictions: Procesar mÃºltiples predicciones en un solo llamado
   - Usar formatos optimizados (ONNX) para inferencia mÃ¡s rÃ¡pida
   - Considerar GPU para modelos grandes

4. **Async/await completo**
   - Convertir todas las llamadas I/O a asÃ­ncronas
   - Utilizar conexiones asyncio para BD y MLflow
   - Implementar connection pooling optimizado

---

## ğŸ“š TecnologÃ­as Utilizadas

- **FastAPI**: Framework web para la API
- **MLflow**: GestiÃ³n del ciclo de vida de modelos ML
- **Locust**: Herramienta de pruebas de carga
- **Docker & Docker Compose**: ContenerizaciÃ³n y orquestaciÃ³n
- **Uvicorn**: Servidor ASGI para FastAPI
- **Python**: Lenguaje de programaciÃ³n principal

---

## ğŸ‘¥ Autores

- **Jonatan Alejandro Gallo**
- **Juan Camilo Torres**
- **Jesus David Romero**

Proyecto desarrollado como parte del curso de MLOps


---

## ğŸ“ Licencia

Este proyecto es parte de un taller acadÃ©mico.

---

## ğŸ”— Enlaces Ãštiles

- [DocumentaciÃ³n de Locust](https://docs.locust.io/)
- [DocumentaciÃ³n de FastAPI](https://fastapi.tiangolo.com/)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [Docker Compose Documentation](https://docs.docker.com/compose/)

