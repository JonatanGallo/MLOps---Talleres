# Taller 5: Pruebas de Carga y Optimizaci√≥n de API con Locust

## üìã Descripci√≥n del Proyecto

Este proyecto tiene como objetivo realizar pruebas de carga a una API de inferencia de Machine Learning utilizando **Locust**, una herramienta de c√≥digo abierto para pruebas de carga distribuidas. La API est√° construida con **FastAPI** y utiliza **MLflow** para servir modelos de predicci√≥n.

El proyecto se compone de dos componentes principales:

1. **API de Inferencia**: Servicio FastAPI que expone un endpoint `/predict` para realizar predicciones de cobertura forestal utilizando un modelo entrenado y almacenado en MLflow.

2. **Locust**: Herramienta de pruebas de carga configurada en modo maestro-trabajador (master-worker) para simular m√∫ltiples usuarios concurrentes realizando solicitudes a la API.

### Estructura del Proyecto

```
taller-5-locust/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml          # Configuraci√≥n de la API de inferencia
‚îú‚îÄ‚îÄ locust/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml          # Configuraci√≥n de Locust (master y workers)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                  # Imagen personalizada de Locust
‚îÇ   ‚îú‚îÄ‚îÄ locustfile.py              # Script de pruebas de carga
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt           # Dependencias de Locust
‚îú‚îÄ‚îÄ images/                         # Capturas de pantalla de los resultados
‚îî‚îÄ‚îÄ README.md
```

## üéØ Objetivo

El objetivo principal de este taller es identificar cuellos de botella en la API de inferencia y aplicar diferentes estrategias de optimizaci√≥n para mejorar el rendimiento, medido en t√©rminos de:

- **RPS (Requests Per Second)**: N√∫mero de solicitudes que la API puede manejar por segundo
- **Latencia**: Tiempo de respuesta promedio
- **Tasa de fallos**: Porcentaje de solicitudes que fallan

---

## üöÄ Proceso de Optimizaci√≥n

A continuaci√≥n se detalla el proceso completo de optimizaci√≥n seguido en este proyecto, con an√°lisis y conclusiones en cada etapa.

### 1. Eliminaci√≥n de Prints en la API

**Problema inicial**: La API conten√≠a m√∫ltiples declaraciones `print()` para debugging que impactaban negativamente el rendimiento de la aplicaci√≥n.

**Soluci√≥n**: Se eliminaron todos los `print()` innecesarios del c√≥digo de la API.

**Resultado**: 
- **Antes**: La API solo alcanzaba **~4 RPS** (Requests Per Second)
- **Despu√©s**: Se logr√≥ una mejora de hasta **~120 RPS** significativa en el rendimiento inicial

**Lecci√≥n aprendida**: Los `print()` son operaciones de I/O s√≠ncronas que bloquean la ejecuci√≥n y pueden degradar significativamente el rendimiento en aplicaciones de producci√≥n, especialmente bajo alta carga. Es fundamental utilizar sistemas de logging apropiados y con niveles adecuados en producci√≥n.

---

### 1.5. Eliminaci√≥n de Llamadas Innecesarias a MLflow

**Problema identificado**: Se detectaron llamadas redundantes a MLflow que no eran estrictamente necesarias para el proceso de inferencia.

**Soluci√≥n**: Se optimiz√≥ el c√≥digo para eliminar las llamadas duplicadas o innecesarias a MLflow, manteniendo solo las operaciones esenciales para la predicci√≥n.

**Resultado**: Reducci√≥n adicional en la latencia de las peticiones al minimizar la comunicaci√≥n con servicios externos.

**Lecci√≥n aprendida**: Cada llamada a servicios externos (MLflow, bases de datos, APIs) a√±ade latencia. Es crucial revisar el c√≥digo para identificar y eliminar operaciones redundantes o que puedan ser cacheadas.

---

### 2. An√°lisis Comparativo de Endpoints

Para identificar espec√≠ficamente d√≥nde se encontraban los cuellos de botella, se realiz√≥ una comparaci√≥n entre dos endpoints:

- **Endpoint vac√≠o**: Un endpoint simple que solo devuelve un string sin procesamiento
- **Endpoint de predicci√≥n**: El endpoint `/predict` que realiza todo el proceso de inferencia

#### 2.1 Resultados y Conclusiones

![Comparaci√≥n entre endpoint vac√≠o y endpoint de predicci√≥n](./images/endpointVacio.png)
*Figura 1: Rendimiento del endpoint vac√≠o*

![Rendimiento del endpoint de predicci√≥n](./images/prediccionSinConBD.png)
*Figura 2: Rendimiento del endpoint de predicci√≥n con llamadas a BD y MLflow*

**Conclusiones del an√°lisis**:

Se identificaron **dos cuellos de botella principales** en el proceso de predicci√≥n:

1. **Llamada a la base de datos**: La API realizaba una consulta a la base de datos en cada petici√≥n para obtener los nombres de las columnas necesarias para el modelo. Esta operaci√≥n a√±ad√≠a latencia significativa.

2. **Llamada a MLflow**: El proceso de inferencia requer√≠a comunicarse con MLflow para cargar el modelo y realizar la predicci√≥n. Esta operaci√≥n remota era el cuello de botella m√°s grande, ya que:
   - Implica comunicaci√≥n de red con el servidor de MLflow
   - Requiere cargar el modelo en cada inferencia
   - El proceso de inferencia en s√≠ mismo toma tiempo

**Prueba sin conexiones externas**:

Para validar el impacto real de las llamadas a la base de datos y MLflow, se realiz√≥ una prueba eliminando completamente estas dependencias:

- **Configuraci√≥n**: Endpoint de prueba que solo normaliza datos sin consultar BD ni MLflow
- **Resultado**: Se alcanzaron aproximadamente **~5000 RPS**
- **Conclusi√≥n**: Las llamadas a servicios externos (BD y MLflow) son responsables de m√°s del **95% de la latencia** en el proceso de inferencia

Esta prueba confirm√≥ que el mayor cuello de botella est√° en las dependencias externas, no en el procesamiento interno de la API.

![An√°lisis de tiempos](./images/soloNormalizarDatos.png)
*Figura 3: An√°lisis detallado de los tiempos de procesamiento*

---

### 3. Implementaci√≥n de R√©plicas y Workers con Docker Swarm

**Estrategia**: Para aumentar la capacidad de procesamiento de la API, se implement√≥ una arquitectura distribuida utilizando:

- **R√©plicas de la API**: M√∫ltiples instancias del contenedor de la API ejecut√°ndose en paralelo
- **Workers de Uvicorn**: M√∫ltiples procesos workers dentro de cada contenedor
- **Docker Swarm**: Orquestaci√≥n de contenedores para distribuir la carga

#### Configuraci√≥n implementada

```yaml
# API con m√∫ltiples workers
command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

```yaml
# Locust en modo distribuido
locust-master:
  command: -f /mnt/locustfile.py --master --expect-workers 8

locust-worker:
  command: -f /mnt/locustfile.py --worker --master-host locust-master
  deploy: {}  # Escalable con: docker compose up --scale locust-worker=8
```

#### Arquitectura Master-Worker de Locust

Una de las optimizaciones clave fue implementar una arquitectura distribuida de Locust usando el patr√≥n **Master-Worker**:

**Componentes**:

1. **Locust Master**: 
   - Coordina la ejecuci√≥n de las pruebas
   - Gestiona la interfaz web (puerto 8089)
   - Distribuye la carga entre los workers
   - Agrega las estad√≠sticas de todos los workers

2. **Locust Workers**:
   - Ejecutan las pruebas de carga reales
   - Se conectan al master
   - Cada worker simula usuarios independientemente
   - Escalables horizontalmente con `--scale`

**Configuraci√≥n del docker-compose de Locust**:

```yaml
locust-master:
  image: locustio/locust
  command: -f /mnt/locustfile.py --master --expect-workers 8
  ports:
    - "8010:8089"
  environment:
    - LOCUST_HOST=http://10.43.100.86:8006
  volumes:
    - ./locustfile.py:/mnt/locustfile.py
  ulimits:
    nofile:
      soft: 1048576
      hard: 1048576

locust-worker:
  image: locustio/locust
  command: -f /mnt/locustfile.py --worker --master-host locust-master
  environment:
    - LOCUST_HOST=http://10.43.100.86:8006
  depends_on: [locust-master]
  volumes:
    - ./locustfile.py:/mnt/locustfile.py
  ulimits:
    nofile:
      soft: 1048576
      hard: 1048576
```

**Escalado de workers**:
```bash
# Levantar Locust con 8 workers
docker compose up --scale locust-worker=8
```

**Ventajas de esta arquitectura**:
- ‚úÖ **Escalabilidad**: Permite simular miles de usuarios concurrentes
- ‚úÖ **Distribuci√≥n de carga**: Los workers distribuyen la carga de forma equilibrada
- ‚úÖ **Flexibilidad**: Se pueden agregar/quitar workers din√°micamente
- ‚úÖ **Mejor utilizaci√≥n de recursos**: Aprovecha m√∫ltiples cores del CPU

#### Optimizaciones en Locust

Para maximizar el rendimiento de las pruebas de carga, se aplicaron las siguientes configuraciones en el archivo `locustfile.py`:

```python
class UsuarioDeCarga(FastHttpUser):
    wait_time = between(4, 9)
    connections = 800         # Pool grande de conexiones por worker
    max_reqs_per_conn = 0    # Sin l√≠mite de requests por conexi√≥n
```

**Mejoras implementadas**:
- **FastHttpUser**: Uso de cliente HTTP optimizado basado en `geventhttpclient`, m√°s r√°pido que el cliente est√°ndar
- **Pool de conexiones grande (800)**: Permite reutilizar conexiones TCP, reduciendo el overhead de crear nuevas conexiones
- **Sin l√≠mite de requests**: Maximiza la reutilizaci√≥n de conexiones al no cerrarlas despu√©s de un n√∫mero fijo de peticiones
- **ulimits aumentados**: Se increment√≥ el l√≠mite de archivos abiertos a 1048576 para soportar miles de conexiones simult√°neas

![Rendimiento con 4 r√©plicas sin workers adicionales](./images/4replicasSinWorkers.png)
*Figura 4: Rendimiento con 4 r√©plicas de la API*

![Rendimiento con 8 workers de Locust](./images/8workers.png)
*Figura 5: Configuraci√≥n con 8 workers de Locust*

**Resultados**:

- ‚úÖ **Mejora significativa**: El uso de r√©plicas y workers increment√≥ considerablemente el RPS y la capacidad de manejar usuarios concurrentes
- ‚ö†Ô∏è **Limitaci√≥n persistente**: A pesar de la mejora, el cuello de botella de las llamadas a MLflow sigue siendo un factor limitante importante
- üìà **Escalabilidad horizontal**: Se demostr√≥ que la API puede escalar horizontalmente, pero con rendimientos decrecientes debido a las dependencias externas

**Desaf√≠os encontrados**:

- Configuraci√≥n de Docker Swarm requiere networking apropiado entre nodos
- Balance de carga entre r√©plicas debe considerar el estado de conexiones de MLflow
- Los recursos del sistema host se convierten en el nuevo l√≠mite

#### Separaci√≥n de Infraestructura: API y Locust en M√°quinas Independientes

**Decisi√≥n arquitect√≥nica cr√≠tica**: Para obtener m√©tricas precisas y evitar la competencia por recursos del sistema, se desplegaron la API y Locust en **m√°quinas separadas**.

**Configuraci√≥n del despliegue**:

```
M√°quina 1 (Servidor API):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  IP: 10.43.100.86:8006         ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   API Container (Swarm)  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - 4 r√©plicas           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - 4 workers c/u        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - 600MB-1GB RAM c/u    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - 1-2 CPU c/u          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

M√°quina 2 (Servidor Locust):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  IP: localhost:8010             ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Locust Master          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - Puerto 8089 (web)    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - Coordina workers     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Locust Workers (x8)    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - Generan carga        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - 800 conexiones c/u   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Beneficios de la separaci√≥n**:

1. **Aislamiento de recursos**:
   - La API usa 100% de los recursos de su m√°quina sin competencia
   - Locust no afecta el rendimiento medido de la API
   - M√©tricas m√°s precisas y confiables

2. **Escalabilidad independiente**:
   - Se pueden agregar m√°s recursos a la API sin afectar Locust
   - Se pueden escalar los workers de Locust sin limitar la API

3. **Simulaci√≥n realista**:
   - Refleja un escenario de producci√≥n donde los clientes est√°n en m√°quinas distintas
   - Incluye latencia de red real entre cliente y servidor

4. **Debugging facilitado**:
   - M√°s f√°cil identificar si los problemas son de la API o del generador de carga
   - Monitoreo independiente de recursos (CPU, RAM, red) en cada m√°quina

**Resultados de la separaci√≥n**:
- ‚úÖ M√©tricas m√°s precisas y reproducibles
- ‚úÖ Capacidad de generar mayor carga sin saturar la m√°quina de la API
- ‚úÖ Mejor identificaci√≥n de los verdaderos cuellos de botella
- ‚úÖ Posibilidad de monitorear recursos de forma independiente

---

### 4. An√°lisis de Asignaci√≥n de Recursos

Se realizaron m√∫ltiples pruebas variando los recursos asignados a cada r√©plica de la API para encontrar la configuraci√≥n √≥ptima.

#### Configuraciones evaluadas

| R√©plicas | Memoria | CPU | Workers Uvicorn | RPS | Observaciones |
|----------|---------|-----|-----------------|-----|---------------|
| 1 | 2GB | 1 | 2 | ~X | Baseline |
| 2 | 1GB | 1 | 2 | ~X | Bueno |
| 4 | 600MB | 1 | 4 | ~X | √ìptimo |
| 4 | 1GB | 2 | 4 | ~X | Mejor rendimiento |

![An√°lisis de recursos 1](./images/recursos-config-1.png)
*Figura 6: Prueba de asignaci√≥n de recursos - Configuraci√≥n 1*

![An√°lisis de recursos 2](./images/recursos-config-2.png)
*Figura 7: Prueba de asignaci√≥n de recursos - Configuraci√≥n 2*

![An√°lisis de recursos 3](./images/recursos-config-3.png)
*Figura 8: Prueba de asignaci√≥n de recursos - Configuraci√≥n 3*

![An√°lisis de recursos 4](./images/recursos-config-4.png)
*Figura 9: Prueba de asignaci√≥n de recursos - Configuraci√≥n 4*

![An√°lisis de recursos 5](./images/recursos-config-5.png)
*Figura 10: Prueba de asignaci√≥n de recursos - Configuraci√≥n 5*

#### Conclusiones de la asignaci√≥n de recursos

**Configuraci√≥n √≥ptima identificada**:

- **Memoria**: Entre **600MB - 1GB** por r√©plica
- **CPU**: Entre **1-2 CPUs** por r√©plica
- **Workers**: La cantidad √≥ptima de workers depende de:
  - N√∫mero de CPUs asignados
  - Memoria disponible
  - Tipo de carga de trabajo (IO-bound vs CPU-bound)

**Recomendaciones**:

1. **Para cargas IO-bound** (como esta API con llamadas a MLflow):
   - Usar m√°s workers (2-4 por CPU)
   - La memoria es m√°s importante que CPUs adicionales

2. **Balanceo workers vs r√©plicas**:
   - M√°s r√©plicas con pocos workers: Mejor aislamiento, mayor overhead
   - Pocas r√©plicas con muchos workers: Mejor uso de recursos, menor overhead

3. **Monitoreo continuo**:
   - Observar uso de CPU y memoria en tiempo real
   - Ajustar seg√∫n patrones de tr√°fico reales
   - Considerar auto-scaling basado en m√©tricas

---

## üê≥ Publicaci√≥n en Docker Hub

Como parte del proyecto, se public√≥ una imagen Docker de la API optimizada en Docker Hub para facilitar su despliegue:

**Imagen publicada**: `jdromero9402/mlops_talleres:inference`

Esta imagen contiene:
- API FastAPI optimizada con todas las mejoras de rendimiento
- Dependencias necesarias para conectar con MLflow
- Configuraci√≥n lista para ejecutarse en producci√≥n

Para usar la imagen:
```bash
docker pull jdromero9402/mlops_talleres:inference
docker run -p 8000:8000 jdromero9402/mlops_talleres:inference
```

---

## üîß Configuraci√≥n y Ejecuci√≥n

### Prerrequisitos

- Docker y Docker Compose instalados en ambas m√°quinas
- Imagen de la API: `jdromero9402/mlops_talleres:inference` (publicada en Docker Hub)
- Conexi√≥n a MLflow configurada
- Conectividad de red entre ambas m√°quinas

### Arquitectura de despliegue

**M√°quina 1 - Servidor de API:**
```bash
# En la m√°quina del servidor (ej: 10.43.100.86)
cd api
docker-compose up -d

# O con Docker Swarm para m√∫ltiples r√©plicas
docker swarm init
docker stack deploy -c docker-compose.yml api-stack
```

**M√°quina 2 - Servidor de Locust:**
```bash
# En la m√°quina de pruebas de carga (diferente a la API)
cd locust

# Modo distribuido (con 8 workers recomendado)
docker-compose up --scale locust-worker=8
```

### Acceder a la interfaz de Locust

Desde cualquier navegador:
- URL: `http://<IP-MAQUINA-LOCUST>:8010`
- Ejemplo: `http://localhost:8010` (si est√°s en la m√°quina de Locust)

### Configurar prueba de carga

En la interfaz web de Locust:
1. **Number of users**: Cantidad de usuarios concurrentes a simular (ej: 1000)
2. **Spawn rate**: Usuarios nuevos por segundo (ej: 50)
3. **Host**: URL de la API en la otra m√°quina: `http://10.43.100.86:8006`

### Comandos √∫tiles

```bash
# Ver logs del master de Locust
docker-compose logs -f locust-master

# Ver estad√≠sticas de workers
docker-compose ps

# Escalar workers din√°micamente
docker-compose up --scale locust-worker=16 -d

# Detener las pruebas
docker-compose down
```

---

## üìä M√©tricas y Resultados

### M√©tricas clave monitoreadas

- **RPS (Requests Per Second)**: Throughput de la API
- **Response Time (ms)**: Latencia promedio, percentil 95, percentil 99
- **Failure Rate (%)**: Porcentaje de peticiones fallidas
- **Number of Users**: Usuarios concurrentes soportados
- **CPU/Memory Usage**: Uso de recursos del sistema

### Evoluci√≥n del rendimiento

| Etapa | Optimizaci√≥n aplicada | RPS aproximado | Mejora |
|-------|----------------------|----------------|--------|
| Inicial | Ninguna (con prints) | ~4 | Baseline |
| 1 | Sin prints | ~15-20 | +275-400% |
| 1.5 | Sin llamadas innecesarias a MLflow | ~25-30 | +525-650% |
| 2 | An√°lisis de bottlenecks | - | Identificaci√≥n |
| 2.1 | **Prueba sin BD/MLflow** | **~5000** | **+124900%** |
| 3 | R√©plicas + Workers | ~50-80 | +1150-1900% |
| 4 | Recursos optimizados + Locust optimizado | ~80-120 | +1900-2900% |

*Nota: Los valores son aproximados y dependen de la configuraci√≥n espec√≠fica y recursos del sistema.*

**Dato importante**: La prueba sin conexiones a base de datos ni MLflow demostr√≥ que la API puede manejar hasta **~5000 RPS**, lo que indica que el procesamiento interno no es el cuello de botella, sino las llamadas a servicios externos.

---

## üéì Lecciones Aprendidas

### 1. Optimizaci√≥n progresiva
- Comenzar con mejoras simples (eliminar prints) que dan resultados inmediatos
- Medir despu√©s de cada cambio para validar el impacto
- Las optimizaciones m√°s complejas no siempre dan los mejores resultados

### 2. Identificaci√≥n de cuellos de botella
- Es fundamental medir y comparar para identificar d√≥nde est√°n realmente los problemas
- Los endpoints de prueba simples ayudan a aislar componentes problem√°ticos
- Las dependencias externas (BD, MLflow) suelen ser los mayores cuellos de botella

### 3. Escalabilidad horizontal
- Docker Swarm y r√©plicas permiten escalar efectivamente
- Sin embargo, si hay cuellos de botella compartidos (MLflow), la escalabilidad es limitada
- El balance entre r√©plicas y workers debe ajustarse al tipo de carga
- La arquitectura Master-Worker de Locust es fundamental para generar carga realista a gran escala

### 4. Asignaci√≥n de recursos
- M√°s recursos no siempre significa mejor rendimiento
- Existe un punto √≥ptimo de recursos que maximiza el costo-beneficio
- El tipo de carga (IO-bound vs CPU-bound) determina qu√© recursos son m√°s cr√≠ticos

### 5. Monitoreo y testing
- Locust es una herramienta poderosa para simular carga realista
- El modo distribuido master-worker permite simular miles de usuarios
- Es importante realizar pruebas con diferentes patrones de carga

### 6. Separaci√≥n de infraestructura
- Desplegar servicios de prueba (Locust) y servicios bajo prueba (API) en m√°quinas separadas es crucial
- Evita la competencia por recursos que puede distorsionar las m√©tricas
- Permite mediciones m√°s precisas y representativas del rendimiento real
- Facilita el debugging al aislar problemas de cada componente

---

## üîÆ Mejoras Futuras

### Optimizaciones de arquitectura

1. **Implementar cach√© de modelos**
   - Cargar el modelo de MLflow una sola vez al iniciar la aplicaci√≥n
   - Mantener el modelo en memoria para todas las inferencias
   - Implementar cache warming al desplegar nuevas r√©plicas
   - **Impacto esperado**: Acercarse a los ~5000 RPS demostrados en las pruebas sin MLflow

2. **Cach√© de metadata**
   - Almacenar nombres de columnas en Redis o en memoria
   - Eliminar completamente las consultas a BD en el hot path
   - Actualizar cach√© solo cuando el schema cambie

3. **Optimizaci√≥n de inferencia**
   - Batch predictions: Procesar m√∫ltiples predicciones en un solo llamado
   - Usar formatos optimizados (ONNX) para inferencia m√°s r√°pida
   - Considerar GPU para modelos grandes

4. **Async/await completo**
   - Convertir todas las llamadas I/O a as√≠ncronas
   - Utilizar conexiones asyncio para BD y MLflow
   - Implementar connection pooling optimizado

---

## üìö Tecnolog√≠as Utilizadas

- **FastAPI**: Framework web para la API
- **MLflow**: Gesti√≥n del ciclo de vida de modelos ML
- **Locust**: Herramienta de pruebas de carga
- **Docker & Docker Compose**: Contenerizaci√≥n y orquestaci√≥n
- **Uvicorn**: Servidor ASGI para FastAPI
- **Python**: Lenguaje de programaci√≥n principal

---

## üë• Autores

- **Jonatan Alejandro Gallo**
- **Juan Camilo Torres**
- **Jesus David Romero**

Proyecto desarrollado como parte del curso de MLOps


---

## üìù Licencia

Este proyecto es parte de un taller acad√©mico.

---

## üîó Enlaces √ötiles

- [Documentaci√≥n de Locust](https://docs.locust.io/)
- [Documentaci√≥n de FastAPI](https://fastapi.tiangolo.com/)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [Docker Compose Documentation](https://docs.docker.com/compose/)

